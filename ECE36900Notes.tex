\documentclass[nobib]{tufte-handout}

%\\geometry{showframe}% for debugging purposes -- displays the margins

\newcommand{\bra}[1]{\left(#1\right)}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=true,factor=1100,stretch=10,shrink=10]{microtype}
\usepackage{color}

% Fixes captions and images being cut off
\usepackage{marginfix}

\usepackage{tikz}
\usepackage{amsmath,amsthm}
\usetikzlibrary{shapes}
\usetikzlibrary{positioning}
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}

% Set up the images/graphics package
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{.}}

\title{Notes for ECE 369 - Discrete Mathematics for Computer Engineering}
\author[Ezekiel Ulrich]{Ezekiel Ulrich}
\date{\today}  % if the \date{} command is left out, the current date will be used

% The following package makes prettier tables.  We're all about the bling!
\usepackage{booktabs}

% The units package provides nice, non-stacked fractions and better spacing
% for units.
\usepackage{units}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

% Small sections of multiple columns
\usepackage{multicol}

% These commands are used to pretty-print LaTeX commands
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name

% Define a custom command for definitions and biconditional
\newcommand{\defn}[2]{\noindent\textbf{#1}:\ #2}
\let\biconditional\leftrightarrow

\begin{document}

\maketitle

\begin{abstract}
These are lecture notes for fall 2023 ECE 36900 at Purdue. Modify, use, and distribute as you please.
\end{abstract}

\tableofcontents

\section{Course Introduction}

This course introduces discrete mathematical structures and 
finite-state machines. Students will learn how to use logical 
and mathematical formalisms to formulate and solve problems in 
computer engineering. Topics include formal logic, proof techniques, 
recurrence relations, sets, combinatorics, relations, functions, 
algebraic structures, and finite-state machines. For more information,
see the syllabus. 

\section{Equations}

\begin{enumerate}
    \item De Morgan's Theorem: 
    
    $\neg(P \wedge Q) \equiv \neg P \vee \neg Q$

    $\neg(P \vee Q) \equiv \neg P \wedge \neg Q$

    \item Modus ponens (mp)
    
    $p$

    $p \rightarrow q$

    $\therefore q$

    \item Modus tonens (mt)
    
    $p \rightarrow q$

    $\neg q$

    $\therefore \neg p$

    \item Predicate inference rules:
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|c|}
        \hline
        Name & Abrv. & Given & Can conclude\\
        \hline
        Existential generalization & eg & $P(a)$ & $(\exists x)P(x)$\\
        \hline
        Existential instantiation & ei & $(\exists x)P(x)$ & $P(a)$\\
        \hline
        Universal generalization & ug & $P(x)$ & $(\forall x)P(x)$\\
        \hline
        Universal instantiation & ui & $(\forall x)P(x)$ & $P(a)$\\
        \hline
        \end{tabular}
    \end{table}

    \item Propositional equivalence rules: 
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|}
        \hline
        Expression & Equivalent to & Name - abbreviation\\
        \hline
        $p \lor q$ & $q \lor p$ & Commutative - comm\\
        $p \land q$ & $q \land p $ &\\
        \hline
        $(p \lor q) \lor r$ &$p \lor (q \lor r)$ & Associative - ass\\
        $(p \land q) \land r$ &$p \land (q \land r)$ &\\
        \hline
        $\neg(p \wedge q)$ & $\neg p \vee \neg q$ & De Morgan's Laws - De Morgan\\
        $\neg(p \vee q)$ & $\neg p \wedge \neg q$ &\\
        \hline
        $p \rightarrow q$ & $\neg p \lor q$ & Implication - imp \\
        \hline
        $p$ & $\neg (\neg p)$ & Double negation - dn \\
        \hline
        $p \biconditional q$ & $(p \rightarrow q) \land (q \rightarrow p)$ & Def'n of equivalence - equ\\
        \hline
        \end{tabular}
    \end{table}
    \item Propositional inference rules: 
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|}
        \hline
        From & Can derive & Name - abbreviation\\
        \hline
        $p, p \rightarrow q$ & $q$ & Modus ponens - mp\\
        $p \rightarrow q, \neg q$ & $\neg p$ & Modus tollens - mt\\
        \hline
        $p, q$ & $p \land q$ & Conjunction - con\\
        \hline
        $p \lor q, \neg p$ & $q$ & Disjunction - dis\\
        \hline
        $p \land q$ & $p, q$ & Simplification - sim\\
        \hline
        $p$ & $p \lor q$ & Addition - add\\
        \hline
        \end{tabular}
    \end{table}
\end{enumerate}

\pagebreak

\section{Propositional Logic}

We often wish that others would be more logical, tell the truth,
or shower. While studying formal logic cannot help with the latter
(in fact, studies have shown a negative correlation between
hygiene and studying formal logic) it is a useful way to 
define what the first two mean. In a formal logic model, we have two constructs:
\begin{itemize}
    \item \defn{Statements/propostion}{A statement or a proposition is a sentence that
    is either true or false.} Propositions are often represented 
    with letters of the alphabet. For example: "$q$: the more time
    you spend coding, the less time you have to buy deodorant."
    \item \defn{Logical connectives}{Used to connect statements.} For 
    example, "and" is a logical connective in English. It can be
    used to connect two statements, e.g. "the person next to me
    smells like dog \emph{and} looks like a dog" to obtain 
    a new statement with its own truth value. 
\end{itemize}

Here are common logical connectives in Boolean logic:
\begin{table}[h]
    \centering
    \begin{tabular}{lc}
    \toprule
    \textbf{Logical Connective} & \textbf{Symbol} \\
    \midrule
    Negation (NOT) & $\lnot$ or $'$ \\
    Conjunction (AND) & $\land$ \\
    Disjunction (OR) & $\lor$ \\
    Exclusive OR (XOR) & $\oplus$ \\
    Implication & $\rightarrow$ \\
    Biconditional & $\leftrightarrow$ \\
    \bottomrule
    \end{tabular}
    \caption{Logical Connectives in Boolean Logic}
    \label{tab:logical-connectives}
\end{table}

\defn{Truth table}{Defines how each of the
connectives operate on truth values. Every connective has one.}
For example, consider $\land$ AND:

\begin{table}[ht]
    \centering
    \begin{tabular}{|c c|c|}
    \hline
    $p$ & $q$ & $p \land q$ \\
    \hline
    T & T & T \\
    T & F & F \\
    F & T & F \\
    F & F & F \\
    \hline
    \end{tabular}
    \caption{Truth table for $\land$}
    \label{tab:truthtableand}
\end{table}
We see that $p$ AND $q$ is only true when $p$ is true and 
$q$ is true. Similarly, $p$ or $q$ is only true when $p$ is true or 
$q$ is true (or both). An important connective for 
discovering new truths is the implication $\rightarrow$, which basically
says "if the first letter is true, then so is the second".
Let $p$: "I live in Wiley" and $q$: "I have no AC". In English,
the statement $p \rightarrow q$ would be stated as "If I live in 
Wiley, then I have no AC". 

\begin{table}[ht]
    \centering
    \begin{tabular}{|c c|c|}
    \hline
    $p$ & $q$ & $p \rightarrow q$ \\
    \hline
    T & T & T \\
    T & F & F \\
    F & T & T \\
    F & F & T\\
    \hline
    \end{tabular}
    \caption{Truth table for $\rightarrow$}
    \label{tab:truthtableimp}
\end{table}
Table \ref{tab:truthtableimp} shows the truth table for $\rightarrow$.
It may not seem immediately clear why, for instance, 
if $p$ and $q$ are false, then $p \rightarrow q$ is true.
If we consider what this means in English, then all we know is 
that I don't live in Wiley. Perhaps I live in Tarkington and still
don't have AC, or perhaps I live in Honors and I do. In any case
the first letter isn't true, so "if the first letter is true
then so is the second" stands as true. If we have the statement 
$p \rightarrow q$, then we call $q$ a \emph{necessary condition}
for $p$. Conversely, $p$ is a \emph{sufficient condition} for $q$.

Say we have a statement such as 
$A \lor B \rightarrow C$.
This is ambiguous, since we can interpret 
it as either $(A \lor B) \rightarrow C$ 
or $A \lor (B \rightarrow C)$. The truth tables
will differ in each case, so it 
becomes necessary to specify in
what order we should apply logical
connectives. 

\begin{enumerate}
    \item Parentheses "()"
    \item Negation "$\neg$"
    \item AND "$\land$"
    \item OR "$\lor$"
    \item Implication "$\rightarrow$"
    \item Biconditional "$\biconditional$"
\end{enumerate}

\section{Rules and proofs}

With each additional variable in your truth table, the number
of choices grows exponentially. Specifically, if you have $n$ statement
letters, you would have $2^n$ choices for your truth table. 

\defn{Tautology}{A formula that is true in every model.} 
Example: I am president of the tautology club because 
I am president of the tautology club. 

\defn{Contradiction}{A formula that is false in every model} Examples:
"it is raining and it is not raining", "I am sleeping and I am awake", 
"IU is a good school". 

\marginnote{Interestingly, it is possible to prove any statement
in a system where a contradiction exists. This is known as the 
\emph{principle of explosion}. To see how it works, consider
the following example: 

\begin{enumerate}
    \item $p$: Donuts are good for you.
    \item $q$: Unicorns exist. 
\end{enumerate}

I'll now assume the contradictory statement "donuts are good for you
and donuts are not good for you".

\begin{align*}
    \neg p \land p &\quad \text{(Given)} \\
    p &\quad \text{(1, simplification)} \\
    p \lor q &\quad \text{(2, addition)}\\
    \neg p &\quad \text{(1, simplification)}\\
    \therefore q &\quad \text{(3, disjunction)}
\end{align*}

Ergo, unicorns exist. 

}

Confusion often arises when negating a sentence such as 
"the book is thick and boring". An natural inclination is to 
negate it thus: "the book is not thick and not boring".
However, consider the truth table for this:
$p$: "the book is thick", $q$: "the book is boring". 
\begin{table}[ht]
    \centering
    \begin{tabular}{|c c|c|c|c|}
    \hline
    $p$ & $q$ & $p \wedge q$ & $\neg(p \wedge q)$ & $\neg p \wedge \neg q$\\
    \hline
    T & T & T & F & F\\
    T & F & F & T & F\\
    F & T & F & T & F\\
    F & F & F & T & T\\
    \hline
    \end{tabular}
    \label{tab:demorganswrong}
\end{table}
We can see the last two rows are not identical, therefore
the negation of "the book is thick and boring" is not 
"the book is not thick and not boring". For $p$ to be 
false, either the book must not be thick \emph{or} the
book must not be boring. This is summarized by 
\defn{De Morgan's Theorem}{
    \[\neg(P \wedge Q) \equiv \neg P \vee \neg Q\]
    \[\neg(P \vee Q) \equiv \neg P \wedge \neg Q\]
}

We now have a sufficient understanding of truth tables and 
logical connectives to come up with some useful rules. First of these 
are 
\defn{Modus ponens (mp)}{
    \begin{align*}
        &p \\
        &p \rightarrow q \\
        &\therefore q \\
    \end{align*}
}

and

\defn{Modus tollens (mt)}{
    \begin{align*}
        &p \rightarrow q \\
        &\neg q \\
        &\therefore \neg p \\
    \end{align*}
}

Below are two tables for commonly used rules.

\begin{table}[ht]
    \centering
    \caption{Equivalence rules}
    \begin{tabular}{|c|c|c|}
    \hline
    Expression & Equivalent to & Name - abbreviation\\
    \hline
    $p \lor q$ & $q \lor p$ & Commutative - comm\\
    $p \land q$ & $q \land p $ &\\
    \hline
    $(p \lor q) \lor r$ &$p \lor (q \lor r)$ & Associative - ass\\
    $(p \land q) \land r$ &$p \land (q \land r)$ &\\
    \hline
    $\neg(p \wedge q)$ & $\neg p \vee \neg q$ & De Morgan's Laws - De Morgan\\
    $\neg(p \vee q)$ & $\neg p \wedge \neg q$ &\\
    \hline
    $p \rightarrow q$ & $\neg p \lor q$ & Implication - imp \\
    \hline
    $p$ & $\neg (\neg p)$ & Double negation - dn \\
    \hline
    $p \biconditional q$ & $(p \rightarrow q) \land (q \rightarrow p)$ & Def'n of equivalence - equ\\
    \hline
    \end{tabular}
    \label{tab:equivalencerules}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Inference rules}
    \begin{tabular}{|c|c|c|}
    \hline
    From & Can derive & Name - abbreviation\\
    \hline
    $p, p \rightarrow q$ & $q$ & Modus ponens - mp\\
    $p \rightarrow q, \neg q$ & $\neg p$ & Modus tollens - mt\\
    \hline
    $p, q$ & $p \land q$ & Conjunction - con\\
    \hline
    $p \lor q, \neg p$ & $q$ & Disjunction - dis\\
    \hline
    $p \land q$ & $p, q$ & Simplification - sim\\
    \hline
    $p$ & $p \lor q$ & Addition - add\\
    \hline
    \end{tabular}
    \label{tab:inferencerules}
\end{table}

At this point, let us formally define an 

\defn{Argument}{An argument can be symbolized as 
\[P_1 \lor P_2 \lor P_3 \lor ... \lor P_n \rightarrow Q\] 
where $P_i$ is called a hypothesis and $Q$ is the conclusion.} If this statement
is a tautology, then the argument is \emph{valid}. 
There are multiple ways we could prove a given argument is a tautology.
For instance, we could create a truth table and brute force an answer.
However, with even four hypotheses this process is tedious, and with each additional hypothesis
it becomes exponentially harder. Therefore we instead often turn to the

\defn{Proof sequence}{a sequence of well-formed formulas 
in which each formula is either a premise or the result 
of applying a derivation rule to earlier well-formed formulas.} In practice this looks like
\begin{align*}
    P_1 \quad &\text{(hypothesis)}\\
    P_2 \quad &\text{(hypothesis)}\\
    P_3 \quad &\text{(hypothesis)}\\
    &\dots \\
    P_n \quad &\text{(hypothesis)}\\
    \text{(formula) }1 \quad &\text{(obtained from derivation rule)} \\
    \text{(formula) }2 \quad &\text{(obtained from derivation rule)} \\
    &\dots \\
    \text{(formula) } n \quad &\text{(obtained from derivation rule)} \\
    \therefore Q \\
\end{align*}

Let's use all this new information in a simple proof. 
\begin{align*}
    A \quad &\text{(hypothesis)}\\
    A \rightarrow B \quad &\text{(hypothesis)}\\
    B \rightarrow C \quad &\text{(hypothesis)}\\
    B \quad &\text{(1, 2, mp)}\\
    C \quad &\text{(4, 3, mp)}\\
    \hline
    \therefore C \notag
\end{align*}
If we wish to apply our knowledge of logic to the real world, 
some practice in translating natural language to formal logic 
is necessary. Let's test it with this statement: "If chicken is on
the menu, then don't order fish, but you should have either fish or salad.
So if chicken is on the menu, have salad." 
Let $C$: "Chicken is on the menu", $F$: "You order fish", and $S$: "You have salad".
We know that if chicken is on the menu you don't order fish, that you should have
either fish or salad, and we'd like to show that if chicken is on the menu 
you should have salad.
\begin{align*}
    C \quad &\text{(hypothesis)}\\
    C \rightarrow \neg F \quad &\text{(hypothesis)}\\
    F \lor S \quad &\text{(hypothesis)}\\
    \neg F \quad &\text{(1, 2, mp)}\\
    S \quad &\text{(3, 4, dis)}\\
    \hline
    \therefore S \notag
\end{align*}

\section{Predicate logic}
\defn{Predicate logic}{Capable of making statements about entire 
groups instead of individual letters. In predicate logic, 
propositions are expressed in terms of predicates, 
variables and quantifiers}, the latter of which propositional logic lacks. 

\defn{Quantifer}{How many objects have a certain property: "for every" or "for some".}

\defn{Predicate}{Property that a variable may have.}

\defn{Domain of interpretation}{Collection of objects from which the variable is taken.}

\defn{Universal quantifer}{"For all": $\forall$}. States that a certain property holds
for all objects in a domain. 

\defn{Existential quantifer}{"There exists": $\exists$}. States that a certain property holds
for at least one object in a domain. 

As an example of a predicate well-formed formula: $(\forall x)[(\exists y) x > y]$. We would read this statment as 
"for all $x$ there exists a $y$ such that $x > y$." At first glance it may seem 
obvious that this statement is true, but consider the domain. What if the domain is 
all natural numbers? Then we could let $x=1$ (or zero depending
or your definition of natural numbers) and there would be no corresponding
lesser $y$. We can see from this example that the truth value of 
a predicate logic formula depends on the domain as well as quantifiers and predicates. 

Just as with propositional logic, we often need to translate English statements
into predicate logic. Take the statement "every movie made by George Lucas is great".
We can rephrase this as "for any movie, if the movie is made by George Lucas, it is great".
We would write this formula as
\[(\forall x)(GL(x) \rightarrow Great(x))\]
(Author's note: no value judgement is associated with this
English statement).

Let's examine some rules in predicate logic. First, negation: 
\[\neg [\forall x A(x)] \biconditional (\exists)x \neg A(x)\]
Some rules from propositional logic still apply in predicate logic. Take 
modus ponens as an example:
\begin{align*}
    (\forall x)(\forall y)L(x,y) \rightarrow [(\exists x) H(x)] \quad &\text{(hypothesis)}\\
    \neg [(\exists x) H(x)] \quad &\text{(hypothesis)}\\
    \neg [(\forall x)(\forall y)L(x,y)] \quad &\text{(1, 2, mt)}\\
    (\exists x) (\exists y) \neg L(x,y) \quad &\text{(3, DM)}\\
    \hline
    \therefore (\exists x) (\exists y) \neg L(x,y) \notag
\end{align*}
Table \ref{tab:predinferencerules} holds predicate inference rules. 
\begin{table}[ht]
    \centering
    \caption{Predicate inference rules}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Name & Abrv. & Given & Can conclude\\
    \hline
    Existential generalization & eg & $P(a)$ & $(\exists x)P(x)$\\
    \hline
    Existential instantiation & ei & $(\exists x)P(x)$ & $P(a)$\\
    \hline
    Universal generalization & ug & $P(x)$ & $(\forall x)P(x)$\\
    \hline
    Universal instantiation & ui & $(\forall x)P(x)$ & $P(a)$\\
    \hline
    \end{tabular}
    \label{tab:predinferencerules}
\end{table}
These rules hold given certain conditions. Namely:
\begin{enumerate}
    \item[(eg)] $x$ not in $P(a)$ 
    \item[(ei)] Must the be first rule that introduces $a$ 
    \item[(ug)] $P(x)$ not derived from a hypothesis
    with $x$ as a free variable, and $P(x)$ is not derived by ei from wff with $x$ as a free variable. 
    \item[(ui)] $a$ is a constant. 
\end{enumerate}
\marginnote{A \emph{free variable} is a variable not bound by a quantifer. For
example, in the formula 
\[(\forall x)(\forall y)P(x,y)\]
both $x$ and $y$ are bound by quantifers. Constrast this with the formula
\[(\exists x)(\forall y)q(x, y, z)\]
In this example, $z$ is a free variable, since it is not associated with 
any quantifiers.}
Let's see some of these rules in action by with a predicate logic proof.
Say we have the statement "every ECE student works harder
than somebody, and everyone who works harder than any other
person gets less sleep than that person. Maria is an 
ECE student. Ergo, Maria gets less sleep than someone. 
Let $E(x)$: "x is an ECE student", $W(x,y)$: "x works harder than y", 
$S(x,y)$: "x gets less sleep than y", and $m$: Maria. 
We want to prove $\exists a (S(m,a))$. 
\begin{align*}
    \forall x, E(x) \rightarrow (\exists y) (W(x,y)) \quad &\text{(hypothesis)}\\
    \forall x, \forall y (W(x,y) \rightarrow S(x,y)) \quad &\text{(hypothesis)}\\
    E(m) \quad &\text{(hypothesis)}\\
    \exists y (E(m) \rightarrow S(m,y)) \quad &\text{(1, ui)}\\
    E(m) \rightarrow W(m,a) \quad &\text{(4, ei)}\\
    \forall y (W(m,y) \rightarrow S(m,y)) \quad &\text{(3, ui)}\\
    %the above line doesn't make sense to me
    W(m,a) \rightarrow S(m,a) \quad &\text{(6, ui)}\\
    W(m,a) \quad &\text{(3,5 mp)}\\
    S(m,a) \quad &\text{(7,8, mp)}\\
    \exists a (S(m,a)) \quad &\text{(9, eg)}\\
    \hline
    \therefore \exists a (S(m,a)) \notag
\end{align*}
%I'm pretty sure this proof is wrong, I'll fix it later
\section{Proofs}
List of common proof techniques:
\begin{enumerate}
    \item Exhaustive proof: 
    In this kind of proof, the statement to be proved is 
    split into a finite number of cases or sets of equivalent 
    cases, and where each type of case is checked to see if 
    the proposition in question holds
    \item Refuting by counter-example: 
    If we have a universal statement such as 
    $\forall x (P(x) \rightarrow Q(x))$, we may
    show it to be false by finding a single $a$ 
    such that $\neg P(a)$. 
    \item Direct proof: 
    Trying to prove $p \rightarrow q$, start by 
    assuming $p$ and then show $q$. 
    \item Proof by contraposition: 
    The contrapositive of $p \rightarrow q$
    is $\neg q \rightarrow \neg p$. A statement 
    and its contrapositive are logically equivalent, so 
    if proving $p\rightarrow q$ is too difficult 
    we may try to prove $\neg q \rightarrow \neg p$ instead. 
    \item Proof by contradiction: 
    Suppose I have to prove $p \rightarrow q$. I can begin 
    by saying $p$ is true and $\neg q$ is true. If by a series of 
    steps I arrive a contradiction, then I may say $p$ implies $q$. 
    \item Proof by induction: employs a neat trick which 
    allows you to prove a statement about an arbitrary number 
    $n$ by first proving it is true when $n=1$ (or some other base case), 
    assuming it is true for $n=k$, and then showing it is true for $n=k+1$.
    The steps to prove $\forall n P(n)$ are:
    \begin{enumerate}
        \item Prove $P(1)$ (this is your \emph{base case}). 
        \item Assume for arbitrary $k \geq 1$, $P(k)$ (your \emph{inductive hypothesis}).
        \item Prove $P(k+1)$. 
    \end{enumerate}
\end{enumerate}
Below are some example proofs using each of these techniques. 
\begin{enumerate}
    \item \emph{Exhaustive proof (cases)}: say we wish to prove $|xy| = |x||y|$. 
    Let's split this into four cases:
    \begin{enumerate}
        \item Case 1: $x$ and $y$ positive.
        Then the absolute values are equal to the original
        numbers, and we have 
        \begin{align*}
            |x| &= x \\
            |y| &= y \\
            |xy| &= xy \\
            |x||y| &= xy \\
            \therefore |xy| &= |x||y|
        \end{align*}
        \item Case 2: $x$ and $y$ negative. 
        If $x$ and $y$ are both negative, then 
        $xy$ is positive. We thus have that 
        \begin{align*}
            |x||y| &= xy \\
            |xy| &= xy \\
            \therefore |xy| &= |x||y| 
        \end{align*}
        \item Case 3: $x$ negative and $y$ positive. 
        Now we have that $xy$ is negative. Still, though, 
        $|xy|$ will be positive (by def'n of ||), and so will 
        $|x|$ and $|y|$. So we again have that 
        \begin{align*}
            |xy| &= |x||y| 
        \end{align*}
        \item Case 4: $x$ positive and $y$ negative. 
        WLOG, case 3. 
    \end{enumerate}
    \begin{align*}
        \therefore |xy| &= |x||y| \qed
    \end{align*}
    \item \emph{Direct proof}: say we wish to prove the product of two even integers is even. 
    We first need to translate this English sentence to a mathematical 
    statement, which we can do in this case like so:
    \[x = 2a, a \in \mathbb{Z}, y = 2b, b \in \mathbb{Z}, \rightarrow x \times y = 2c, c \in \mathbb{Z}\]
    Our proof is below. 
    \begin{align*}
        x &= 2a, a \in \mathbb{Z} \\
        y &= 2b, b \in \mathbb{Z} \\
        z &= x \times y \\
        &= 2a\times 2b \\
        &= 2(2ab) \\
        &= 2c, c \in \mathbb{Z} \qed
    \end{align*}
    Since $c$ is an integer, $2c$ is even and the proof is complete. Try proving 
    the product of two odds is odd in a similar fashion. 
    \item \emph{Proof by contradiction}: say we wish to prove $\sqrt{2}$ is irrational. In a theme
    that will become common as we see more proofs by contradiction, 
    assume the opposite. That is, assume $\sqrt{2}$ is rational. By
    the definition of rational, we can then write 
    \[\sqrt{2} = \frac{a}{b}, a,b \in \mathbb{Z}\]
    Where $a$ and $b$ share no common factors. 
    We can then perform the following series of steps. 
    \begin{align*}
        \sqrt{2} &= \frac{a}{b} \\
        b\sqrt{2} &= a \\
        2b^2 &= a^2
    \end{align*}
    This means that $a^2$ is even. It can be easily shown 
    that if $a^2$ is even then $a$ is even. That means that 
    $a^2$ will actually be divisible by 4. We can rearrange to get
    \begin{align*}
        2b^2 &= 4c, c \in \mathbb{Z} \\
        b^2 &= 2c
    \end{align*}
    So $b$ is likewise even. But if both $a$ and $b$ are even, 
    then they share a common factor and our original supposition is false. 
    Ergo $\sqrt{2}$ is irrational. $\qed$. 
    \item \emph{Poof by induction}: say we wish to show 
    \[\sum_{i = 1}^{n} = \frac{n(n+1)}{2}\]
    Begin with the base case $n=1$. 
    \begin{align*}
        \sum_{i = 1}^{1} &= 1 \\
        &= \frac{1(1+1)}{2} \\
        &= \frac{n(n+1)}{2}
    \end{align*}
    Since we have shown the base case to be true, 
    we may now make our inductive hypothesis and assume that for
    arbitrary $k \geq 1$, 
    \begin{align*}
        \sum_{i = 1}^{k} &= \frac{k(k+1)}{2}
    \end{align*}
    Let us now show that the formula holds for $k + 1$. 
    \begin{align*}
        \sum_{i = 1}^{k + 1} &= \sum_{i = 1}^{k} + (k + 1)\\
        &= \frac{k(k+1)}{2} + (k + 1) \\
        &= \frac{k(k+1) + 2(k+1)}{2}\\
        &= \frac{k^2+3k+2}{2}\\
        &= \frac{(k + 1)(k + 2)}{2} \\
        &= \frac{(k + 1)((k+1) + 1)}{2} 
    \end{align*}
    And we are done $\qed$. 
\end{enumerate}
The form of induction we have just seen is the \emph{weak} form of 
induction. The \emph{strong} form of induction is the following.
\marginnote{The strong form is also known as the \emph{second principle of mathematical induction}}
To prove $\forall x, P(x)$, we still prove the base case ($P(n)$). 
Now, however, we assume for arbitrary $k \geq 1$ that $P(r)$ is 
true for $1 \leq r \leq k$ and try to prove $P(k+1)$.
Let's see this in action. Say we'd like to prove any postage 
greater than or equal to 8 cents can be created with a combination 
of 3 cent and 5 cent postage stamps. First, the base case. 8 can be 
created like so: $3+5 = 8$. Now let's assume for all $8 \leq r \leq k$, 
$P(r)$. Now the tricky part. To prove $P(k+1)$, notice that
\begin{align*}
    P(k+1) &= k+1 \\
    &= (k-2) + 3 \\
    &= (3a+5b) + 3 \\
    &= 3c + 5b \qed
\end{align*}
We had to rewrite $k+1$ as $k-2 + 3$ so we could 
use our assumption that $P(k-2)$ is true. 
\marginnote{We see that simply assuming $P(k)$
wouldn't be sufficient in this case, so there are proofs where we can 
use strong induction but not weak induction. Anything that can be proved
with weak induction can be proved with strong induction, since in 
strong induction we assume $P(k)$ in addition to $P(r)$ for all $r$ between 
$1$ and $k$. Hence, "strong".}
The astute among you will recognize that our proof is technically
imcomplete. We have assumed $P(k-2)$, but what if we wish to prove $P(9)$? 
This is not included in our inductive step, since we only assume $P(r)$ 
for $8\leq r \leq k$. We are in the domain of natural numbers (unless you
have somehow managed to find a postage stamp with negative or fractional value),
then there is no $r$ for which this is true. Therefore we also need to prove 
$P(9)$ and $P(10)$, which is pretty simple. 

Let's see a more advanced example 
of induction. Say you have a set of $n$ elements, and you want to create subsets of this set. 
You are interested in knowing how many subsets exist. After trying it out with 
$n=1,2,3$ you suspect the number of subsets that can exist is $2^n$, and you see
that this problem is a good candidate for induction. Your base case is 
$P(1) = 2 = 2^1$, so that's out of the way. Now assume $P(k)$. That is, for any 
set with $k < n$ elements, the number of subsets is $2^k$. You now need to show 
$P(k+1)$, which can be done by noticing that if we add an additional element to the set, 
then for every pre=existing group you can add the new element to get $2^k$ new groups, bringing your total 
number of subsets to $2^k+2^k=2\times2^k = 2^{k+1}$, and we are done.  

We've seen some examples of when induction is used well, but doing everything 
perfectly is tiring. Let's show something false: any group of horses are 
all the same color. Obviously the base case is true: a group of one horse 
is always the color of that horse. Assume $P(k)$, that any group of $k$ horses 
is monochromatic. Now we need to prove it for $k+1$. First, exclude one horse and 
look only at the other $k$ horses; all these are the same color, since $k$ horses 
always are the same color. Likewise, exclude some other horse (not identical to 
the one first removed) and look only at the other $k$ horses. By the same reasoning, 
these, too, must also be of the same color. Therefore, the first horse that 
was excluded is of the same color as the non-excluded horses, who in turn are 
of the same color as the other excluded horse. Hence, the first horse excluded, 
the non-excluded horses, and the last horse excluded are all of the same color, 
and we are done. What's the issue here? The issue is that in order to select two different 
horses, we need at least two different horses. However, our base case was with one horse. 
To use this logic we would need to show $P(2)$, for which our original argument is
obviously not true. 

\section{Proofs of correctness}

Say you are part of a team developing software for the NSA or NASA. 
A common requirement for the deliverable is to prove that it meets 
certain properties. You may be asked to show that your code 
will eventually return an answer (i.e. the algorithm terminates)
or that, \emph{if} an answer is returned, it will be correct. 
Correct in this sense means your program meets whatever specifications 
have been laid out. 

Broadly speaking, there are two properties that a program must 
satisfy. The first is 
\defn{safety}{the program will not violate any invariant that you write 
(think \texttt{assert} for C programs. If you always need to return 
a positive integer, for instance, and you can show that it always does, 
then you have satisfied safety)}. The second is 
\defn{liveness}{that your program is going to terminate}. This can be 
done by showing there are no infinite loops, for example. You may also specify
the amount of time in which your program is guarenteed to terminate. 
In the field, the longest possible time your program will take 
is known as Worst Case Execution Time (WCET).

In software development, as you are likely aware, your code is often checked 
using tests. This is one method of gaining evidence that the kernel of your 
program is correct. 
\marginnote{Note that "testing can prove the presence of errors 
but never their absence". We cannot show through testing that a program
will behave exactly as it should in all cases, unless the number of possible 
inputs is so small that it is possible to enumerate through them. Even though 
testing cannot prove the correctness of a program, it can still reveal issues
and build confidence that the code is correct. }
We may also prove that the code is correct, which is 
often more laborious. Unless you're developing software for safety-critical 
areas, you'll probably use testing more than formal proof. If you needed 
to unequivocally show that your program will do what it 
should under all circumstances of interest, you'll have to use formal 
logic techniques. 

To understand how we can apply logical rules to programming, imagine 
your algorithm as a function $P$ that takes in some input values $X$
and produces some output values $Y$. 
\[Y = P(X)\]
The predicate $Q(X)$ describes conditions that the input values 
will satisfy (e.g. $X > 0$, $X \in \mathbb{R}$, $X$ is a string under 127
characters). The predicate $R(X,Y)$ describes conditions that the output must satisfy
for a given input (e.g. $Y = \sqrt{X}$, $Y$ gives the prime factorization of $X$, 
$Y$ capitalizes $X$). We say a program is correct if the implication
\[\forall X[Q(X) \rightarrow R(X,P(X))]\]
is valid. That is, whenever $Q(X)$ is true of the inputs, $R(X,Y)$ should 
be true of the outputs. The notation changes when dealing with 
program correctness: here's the more common way to write the above implication:
\[\{Q\}P\{R\}\]
The terminology here is that $Q$ is a \emph{precondition} for program $P$, 
and $R$ is the \emph{postcondition}.
\marginnote{
$\{Q\}P\{R\}$ is called a \emph{Hoare triple} and gives 
before and after conditions for a program fragment. For instance, 
precondition: $Q(x)$, program: $Y = P(x)$, postcondition: $R(x,y)$
is a Hoare triple that means 
\begin{align*}
    (\forall X) Q(X) &\rightarrow R(X,Y) \\
    (\forall X) Q(x) &\rightarrow R(X, P(X))
\end{align*}
}

\defn{Precondition}{a condition that is true before the execution of a program}.

\defn{Postcondition}{a condition that is true after the execution of a program}.

A program is broken down into individual statements $s_i$, with predicates
sandwiching them.
\marginnote{These sandwiching predicates are also called \emph{assertations}, 
because having multiple names for the same concept makes it much more fun to learn.}
Here is the general form of a predicate-statement sandwich: 
\begin{align*}
    \{Q\} \\
    s_0 \\
    \{R_1\} \\
    s_1 \\
    \{R_2\} \\
    \dots \\
    s_{n-1} \\
    \{R\}
\end{align*}
$Q, R_1, R_2, R_n = R$ are assertations. Your program $P$ is provably correct if
each of the following implications holds: 
\begin{align*}
    \{Q\}s_0\{R_1\} \\
    \{R_1\}s_1\{R_2\} \\
    \dots \\
    \{R_{n-1}\}s_{n-1}\{R\}
\end{align*}
So to prove correctness for $P$, "all" you need to do is 
produce this sequence of valid implications. 

With the idea of correctness hopefully clear, let me introduce you to 
\emph{assignment statements}. An assignment statement is something 
with an equals, like $x = a$.
Assignment statements often come with a postcondition. 
It will be your job to find 
the precondition that makes it true. For example, say you have 
the assignment statement $y = x + 1$ and the postcondition $y = 10$. 
What is the precondition that makes this true? What is the Hoare triple?

The appropriate rule of inference for assignment statements is the 
\defn{assignment rule}{states that $\{R_i\}s_i\{R_{i+1}\}$ is valid 
provided $s_i$ is an assignment statement ($x = a$) and $R_i$ is 
$R_{i+1}$ with $a$ substituted everywhere for $x$.}

\marginnote{
In proving code correctness, we often put arithmetically equivalent 
assertations in sequences with no lines of code in between, like 
\begin{align*}
    \{y &= 4\} \\
    \{y + 10 &= 14\} \\
    x &= y + 10 \\
    \{x &= 14\}
\end{align*}
}

Let's put all of this into practice with an example. Say we 
wish to prove the following computes $x(x-1)$ correctly. 
\begin{lstlisting}[caption=Assignment rule example]
    y = x - 1
    y = x * y
\end{lstlisting}
To show this, we can use the assignment rule with \texttt{y = x - 1}
and plug it into \texttt{y = x * y} to get \texttt{y = x * (x - 1)}. 
The key principle here is that you can use proof rules to show 
that a postcondition holds for a given set of preconditions. As long 
as the given set of preconditions is a subset of the derived preconditions, 
you're good and the postconditions will be met. 

Let's examine conditional statements now. 
Proving a conditional statement "$\{Q\}$ if $B$ then $P_1$ else $P_2$ $\{R\}$"
boils down to showing two things:
\begin{enumerate}
    \item $\{Q$ and $B\}$ $P_1$ $\{R\}$
    \item $\{Q$ and $\neg B\}$ $P_2$ $\{R\}$
\end{enumerate}
Say we have something like the following:
\begin{lstlisting}[caption=Example]
    {x = 7}
        if x <= 0 
            y = x
        else
            y = 2*x 
    {y = 14}
\end{lstlisting}
We must show each of the two cases, that 
\begin{enumerate}
    \item $\{x = 7 \land x \leq 0\}$ $y = x$ 
    \item  $\{x = 7 \land x > 0\}$ $y = 2x$ 
\end{enumerate}
We can use the assignment rule with \texttt{x = 7} to show this is true. 
In case 2, we have \texttt{$y = 2\times7 = 14$}, so our postcondition is true. In case 1, 
$x$ is not less than or equal to zero, so we have that $y$ is true by the defn of implication. We 
have then shown that both are cases are true and we are done. 

Let's see another example of a conditional proof. We want to verify 
the correctness of this code block: 
\begin{lstlisting}[caption=Example]
    {x = 11}
        y = x - 1
    {y = 10}
        if x <= 0
            z = y - 1
        else 
            z = y + 3
    {z = 13}
\end{lstlisting}
We must show each of the two cases again. 
\begin{enumerate}
    \item $\{y = 10 \land y \leq 0\}$ $z = y - 1$ \{z = 13\}
    \item  $\{y = 10 \land y > 0\}$ $z = y + 3$ \{z = 13\}
\end{enumerate}
Again, since $10 \nleq 0 $, the first case is true. 
For the second, use the assignment rule.  
\begin{align*}
    y + 3 &= 13 \text{ by assignment rule}\\
    z &= y + 3 \\
    z &= 13
\end{align*}
So the second case is true as well. We ought to show via 
the assignment rule that \{y = 10\} and \texttt{y = x - 1}
gives us the derived precondition \texttt{x = 11}, which is a subset 
of our given precondition, but we are lazy so we won't do that. 

Let's look at something a lot more interesting: loop statements. 
\begin{lstlisting}[caption=Example]
    while B
        S
\end{lstlisting}
The strategy here is to repeatedly perform statement S until B is true. 
That is, 
\begin{align*}
    \{Q\}\{R\} \\
    \{Q\}S\{R\} \\
    \{Q\}S;S\{R\} \\
    \dots
\end{align*}
Covering every case: that the loop executes no times, once, etc. 
Doing this by cases would be exhausting, so let's use induction instead. 
We must find a \emph{loop invariant}, a statement that is true no matter 
how many times the loop executes. Then we must show that the loop invariant and 
not B implies the conclusion we want to verify. 
\begin{lstlisting}[caption=Loop invariant]
    Sum(n)
    i = 1;
    j = 0;
    while i != n 
        j = j + i
        i = i + 1
\end{lstlisting}
What's something that's true here no matter the number of times the loop
executes? How about j = 0 + 1 + ... + (i - 1)? Since this program 
is supposed to calculate the sum from 0 to n - 1, this would 
be a useful thing to prove. To show this is true, 
we can use induction. The base case is when the loop executes not at all, 
so j = 0 and i = 1. It is definitely true that 0 = 1 - 1, so the base 
case works. Now we assume jk = 0 + 1 + ... (ik - 1), and try to show 
that j(k+1) = 0 + 1 + ... (i(k+1) - 1). Try this yourself: it is a good exercise 
in induction. Once we have proved this, then we have to show that the loop 
invariant and not B (i = n) gives us the desired result, which in this case 
is the sum from 0 to n - 1. 

Let's try another example. 
\begin{lstlisting}[caption=Loop invariant]
    {a >= b, not both zero}
    GCD(a, b)
    i = a;
    j = b;
    while j != 0
        r = i mod j 
        i = j 
        j = r 
    {i = gcd(a,b)}
\end{lstlisting}
Here's the loop invariant: gcd(i,j) = gcd(a,b). We know not B 
is j = 0. Therefore, gcd(i,0) = gcd(a,b) = i. Now we just need 
to prove that the loop invariant is true, again usign induction. 
The hardest part of these problems is coming up with a loop invariant, 
after that step we just apply induction. 
\end{document}
